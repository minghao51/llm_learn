{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.environ.get(\"GROQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low latency language models (LLMs) are important in the field of natural language processing (NLP) and artificial intelligence (AI) due to several reasons:\n",
      "\n",
      "1. Real-time applications: Low latency is crucial for real-time applications such as chatbots, voice assistants, and real-time translation services. Users expect immediate responses from these systems, and high latency can result in a poor user experience.\n",
      "2. Improved user experience: Low latency LLMs can provide a more seamless and natural user experience. Users are more likely to engage with a system that responds quickly and accurately, leading to higher user satisfaction and retention.\n",
      "3. Better decision-making: In some applications, such as financial trading or autonomous vehicles, low latency is critical for making split-second decisions. LLMs that can quickly process and analyze large amounts of data can provide a competitive advantage in these domains.\n",
      "4. Scalability: Low latency LLMs can handle a higher volume of requests and scale more efficiently. This is important for large-scale applications that require fast and efficient processing of large amounts of data.\n",
      "5. Cost-effective: Low latency LLMs can reduce the cost of running and maintaining NLP and AI systems. High latency can result in increased infrastructure costs, as more resources are required to handle the increased processing time.\n",
      "\n",
      "Overall, low latency LLMs are essential for building efficient, scalable, and high-performing NLP and AI systems. They can improve user experience, decision-making, and cost-effectiveness, making them an essential component of modern NLP and AI applications.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=GROQ_API_KEY,\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of low latency LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Via Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Low Latency Large Language Models (LLMs) are critical in many applications due to several reasons:\\n\\n1. Real-time interaction: Low latency LLMs can process user inputs quickly, providing real-time interaction, which is essential for applications such as chatbots, voice assistants, and online gaming. Users expect immediate responses, and high latency can lead to a poor user experience.\\n2. Improved user engagement: Low latency LLMs can maintain user engagement by providing quick and accurate responses. High latency can cause users to lose interest, leading to a poor user experience and reduced engagement.\\n3. Enhanced accuracy: Low latency LLMs can improve the accuracy of the generated responses. When LLMs take too long to process user inputs, the context of the conversation can be lost, leading to inaccurate or irrelevant responses.\\n4. Better decision-making: Low latency LLMs can provide real-time insights and recommendations, enabling better decision-making. For instance, in financial trading or healthcare applications, low latency LLMs can help make critical decisions quickly, leading to better outcomes.\\n5. Competitive advantage: Low latency LLMs can provide a competitive advantage in industries where real-time processing and decision-making are critical. For instance, in online gaming or e-commerce, low latency LLMs can help businesses stay ahead of their competitors by providing a faster and more responsive user experience.\\n\\nIn summary, low latency LLMs are essential for providing a fast, accurate, and engaging user experience. They can improve decision-making, enhance user engagement, and provide a competitive advantage in various industries.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Client\n",
    "chat = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Setup\n",
    "# Write a prompt and invoke ChatGroq to create completions:\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "response = chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Via Langchain + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low latency Large Language Models (LLMs) are important in certain applications due to their ability to process and respond to inputs quickly. Latency refers to the time delay between a user's request and the system's response. In some real-time or time-sensitive applications, low latency is crucial for providing a good user experience and ensuring that the system can respond to changing conditions in a timely manner.\n",
      "\n",
      "For example, in conversational agents or chatbots, low latency is important for maintaining the illusion of a real-time conversation. If there is a significant delay between the user's input and the agent's response, it can disrupt the flow of the conversation and make it feel less natural. Similarly, in applications such as online gaming or financial trading, low latency is critical for enabling users to make decisions and take actions quickly based on real-time data.\n",
      "\n",
      "Moreover, low latency LLMs can help reduce the computational cost of running large language models. By reducing the amount of time that the model spends processing each input, it is possible to serve more requests within a given amount of time, or to use fewer resources to serve the same number of requests. This can make large language models more practical and cost-effective to deploy in real-world applications.\n",
      "\n",
      "Overall, low latency LLMs are important for enabling real-time or time-sensitive applications to provide a good user experience, make decisions quickly, and reduce computational cost.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of low latency LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

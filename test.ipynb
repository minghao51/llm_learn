{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.environ.get(\"GROQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low latency Large Language Models (LLMs) are important in certain applications due to their ability to process and respond to inputs quickly. Latency refers to the time delay between a user's request and the system's response. In some real-time or time-sensitive applications, low latency is crucial for providing a good user experience and ensuring that the system can respond to changing conditions in a timely manner.\n",
      "\n",
      "For example, in conversational agents or chatbots, low latency is important for maintaining the illusion of a real-time conversation. If there is a significant delay between the user's input and the agent's response, it can disrupt the flow of the conversation and make it feel less natural. Similarly, in applications such as online gaming or financial trading, low latency is critical for enabling users to make decisions and take actions quickly based on real-time data.\n",
      "\n",
      "Moreover, low latency LLMs can help reduce the computational cost of running large language models. By reducing the amount of time that the model spends processing each input, it is possible to serve more requests within a given amount of time, or to use fewer resources to serve the same number of requests. This can make large language models more practical and cost-effective to deploy in real-world applications.\n",
      "\n",
      "Overall, low latency LLMs are important for enabling real-time or time-sensitive applications to provide a good user experience, make decisions quickly, and reduce computational cost.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = Groq(\n",
    "    api_key=GROQ_API_KEY,\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of low latency LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Low Latency Large Language Models (LLMs) are critical in many applications due to several reasons:\\n\\n1. Real-time interaction: Low latency LLMs can process user inputs quickly, providing real-time interaction, which is essential for applications such as chatbots, voice assistants, and online gaming. Users expect immediate responses, and high latency can lead to a poor user experience.\\n2. Improved user engagement: Low latency LLMs can maintain user engagement by providing quick and accurate responses. High latency can cause users to lose interest, leading to a poor user experience and reduced engagement.\\n3. Enhanced accuracy: Low latency LLMs can improve the accuracy of the generated responses. When LLMs take too long to process user inputs, the context of the conversation can be lost, leading to inaccurate or irrelevant responses.\\n4. Better decision-making: Low latency LLMs can provide real-time insights and recommendations, enabling better decision-making. For instance, in financial trading or healthcare applications, low latency LLMs can help make critical decisions quickly, leading to better outcomes.\\n5. Competitive advantage: Low latency LLMs can provide a competitive advantage in industries where real-time processing and decision-making are critical. For instance, in online gaming or e-commerce, low latency LLMs can help businesses stay ahead of their competitors by providing a faster and more responsive user experience.\\n\\nIn summary, low latency LLMs are essential for providing a fast, accurate, and engaging user experience. They can improve decision-making, enhance user engagement, and provide a competitive advantage in various industries.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Client\n",
    "chat = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Setup\n",
    "# Write a prompt and invoke ChatGroq to create completions:\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "response = chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " '_lc_kwargs',\n",
       " 'additional_kwargs',\n",
       " 'construct',\n",
       " 'content',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'example',\n",
       " 'from_orm',\n",
       " 'get_lc_namespace',\n",
       " 'id',\n",
       " 'is_lc_serializable',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_id',\n",
       " 'lc_secrets',\n",
       " 'name',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'pretty_print',\n",
       " 'pretty_repr',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'type',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low latency Large Language Models (LLMs) are important in certain applications due to their ability to process and respond to inputs quickly. Latency refers to the time delay between a user's request and the system's response. In some real-time or time-sensitive applications, low latency is crucial for providing a good user experience and ensuring that the system can respond to changing conditions in a timely manner.\n",
      "\n",
      "For example, in conversational agents or chatbots, low latency is important for maintaining the illusion of a real-time conversation. If there is a significant delay between the user's input and the agent's response, it can disrupt the flow of the conversation and make it feel less natural. Similarly, in applications such as online gaming or financial trading, low latency is critical for enabling users to make decisions and take actions quickly based on real-time data.\n",
      "\n",
      "Moreover, low latency LLMs can help reduce the computational cost of running large language models. By reducing the amount of time that the model spends processing each input, it is possible to serve more requests within a given amount of time, or to use fewer resources to serve the same number of requests. This can make large language models more practical and cost-effective to deploy in real-world applications.\n",
      "\n",
      "Overall, low latency LLMs are important for enabling real-time or time-sensitive applications to provide a good user experience, make decisions quickly, and reduce computational cost.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of low latency LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
